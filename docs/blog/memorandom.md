# 备忘录

## qr-dqn 学习与热传导衰减

价值头输出使用概率分布代替单值

使用离散阶概率来逼近连续概率分布, 如 $[0, \frac{1}{10}), [\frac{1}{10}, \frac{2}{10}), ..., [\frac{9}{10}, 1]$

推导时使用精度权重合并子节点概率分布

\[ \frac{1}{\sigma^2} = \sum_i \frac{1}{\sigma_i^2} \]

\[ \mu = \sigma^2 \sum_i \frac{\mu_i}{\sigma_i^2}, \]

注意, 节点保存 $\frac{1}{\sigma}$ 来避免浮点数频繁取倒数引入的计算成本和浮点误差.

训练时使用离散化热传导来将终局精确胜利概率逐渐衰减为平均概率,

\[ \frac{\partial u}{\partial t} = \alpha \nabla^2u \Longrightarrow
  \frac{u^{(t+1)}_i - u^{(t)}_i}{\Delta t} = \alpha \frac{\Delta^2 u^{(t)}_i}{\Delta x^2}, \]

其中 $\Delta$ 是差分算子.

并且有边界条件

\[ \frac{\partial u}{\partial x} \Big|_{u=0} = \frac{\partial u}{\partial x} \Big|_{u=1} = 0
\Longrightarrow \Delta u_0 = \Delta u_{n} = 0. \]

特别地, 能量守恒(边界条件保证体系绝热, 无输入/输出热流)保证了所有时刻热量和与初值和保持一致为1, 即保证了概率归一化.

![胜一目](pic/1.png)
![胜五目](pic/5.png)
![胜十目](pic/10.png)

待验证:

* 训练效率如何, 是否会导致拟合缓慢甚至无法收敛?

* 过度平均化是否会导致中局概率高度平庸式均匀?

* 是否能缓解甚至解决大胜/大败局面 V(s) 过于接近导致策略退化?

* 是否有助于改进网络性能?

* 是否可以使用根节点 $\sigma$ 代替 UCB 公式中的探索项, 优先探索大标准差的结点?

* 标准差是否会在计算过多次时数值不稳定?

  * 如果有, UCB 原探索项的信息论背景/分布假设是什么? 是每次测量服从标准差不依赖于局面的正态分布吗? 使用标准差改进后多大程度上比这种假设更精确?

* 热传导衰减是否有助于改进基于单点传导的概率估计导致的稀疏化问题? 和传统的在目标上添加一个低权高斯分布做法有何优劣差异?

* 热传导衰减是否有可解释的数学背景?

## 差分最优 q

使用差分最优 q 值是否有助于改进各种策略条件下模型的策略选择? 对于根节点, 将所有 q 进行归一化 (线性归一化或高斯归一化) 是会放大单步不同动作的效益收益, 削减概率退化, 还是会在 q 接近时放大估计噪声, 甚至因为对单个动作的探索会影响其它动作 q 值估计而使得噪声震荡?

## 平滑化胜率

对胜目数进行平滑化来设置胜率目标 (例如 sigmoid) 而不是在 -0.5 目 到 0.5 目处越阶式设置胜率目标, 是否有助于缓解对胜率的极端估计? 还是会削弱模型在低目差时的判断力, 从而使策略退化?

## 自适应注意力混合和旁路周期增益块

详见模型中的 ParamAttentionMixing 和 BypassCycleGainBlock (暂未实装, 须在 ai-BCGB 分支中查看)

## pass 输出

使用一个 361 -> ... -> 1 的小线性网络输出 pass 概率. 一方面避免了 361 -> ... -> 362 导致过大且无用连接居多的策略头, 又对齐了围棋合理的输出空间. 并且根据当前各点的 "推荐情况(策略输出)" 来得到pass概率, 有助于模型通过 pass target 高的情况下学习当前局面不够好所以要降低推荐, 或许一定程度有利于过拟合. 本质上通过给 softmax 前的 logits 做了连接, 建立了一个残差连接, 有助于模型看到 softmax 归一的概率背后的东西.
