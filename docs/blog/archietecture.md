# 架构

## 树搜索

使用公式

\[ ucb(a, s) = Q(s_{\overset a \rightarrow}) + c \frac {\sqrt{N(s)}} {N(s_{\overset a \rightarrow}) + 1} P(a | s) \]

指导搜索, 其中, $ N(s) $ 为状态 $s$ 的访问次数,

\[
  Q (s) =
  \begin{cases}
  0 & \text{if s is not visited} \\
  V(s) & \text{if s is a leaf node} \\
  \sum_{l \in leaf(s)} V(l) = \sum_{c \in child(s)} \frac{N(c)}{N(s)} Q(c) & \text{otherwise}
  \end{cases}
\]

为蒙特卡洛价值估计, $ V(\cdot) $ 为神经网络价值估计, $ P(\cdot | s) $ 为神经网络在一定局面下的策略估计. 选取 ucb 最大的动作直至未探索分支, 计算并回馈更新.

该公式更鼓励探索, 适用于神经网络提供了充分多的先验信息的情况.

## 训练周期

采用经验池. 容纳 10w~50w 条经验, 以单条经验 loss 为键排序.

每个 epoch:

1. **准备阶段**. 生成经验池容量 2~5% 的经验, 并淘汰掉超过经验池容量的 loss 最小的经验.

2. **训练阶段**. 采样经验池中 loss 最大的经验, 以 64~128 的 batch size 进行训练, 并在每个 batch 后更新采样的经验的 loss 并重新排序经验池. 训练直到采样经验数总和超过经验池容量的 10% ~ 15%. 跟踪 loss 和采样经验 loss 变化, 跟踪经验池重排序状况.

3. **检验阶段(Optional)**. 和 epoch 前备份的模型进行若干场比赛, 跟踪目差和胜率.

## 策略网络损失

使用蒙特卡洛分布作为策略网络目标, 每一步至少蒙特卡洛采样 30000~3000(依训练阶段不同而定). 使用交叉熵作为损失函数.

\[ Pr (\cdot | s) = \text{softmax} P(\cdot|s)\]

\[ \text{MCTS}(\cdot|s) = \frac{\text{MCTS visit times of } \cdot}{\sum _ a\text{MCTS visit times of } a} \]

\[ \text{loss} = - \sum _ a \text{[a is legal]}\text{MCTS}(a|s) \log Pr(a|s). \]

> 或许在某些时候可以使用策略梯度损失 $ \text{loss} = - A(s, a) \log P(a|s) $ ?

## 价值网络损失

价值网络目标为蒙特卡洛模拟得到的 Q 值和终局奖励值 z 的加权和, 由 $\alpha$ 控制, 其中 z 在胜利时设置为 1, 失败时设置为 0. 并使用 MSE 作为损失函数.

\[
  \text{target} = \alpha Q(s) + (1 - \alpha) z \\
  \text{loss} = (V(s) - \text{target})^2.
\]

开始训练时, $\alpha$ 设置的较小 (0.05 ~ 0.1), 随着训练进展逐渐提高 (0.3 ~ 0.5).

> 引入奖励衰减 $z' = \gamma^t z$ ?

## 混合训练策略

策略网络损失和价值网络损失加权和得到总损失, 二者的损失尺寸应该对齐. 由于策略网络目标空间大, 而价值网络需要保持相对稳定, 所以价值网络的占比不应超过总损失的 20%. ($\gamma \leq 0.2$)

\[ \text{loss} = \gamma\times value\_loss' + (1 - \gamma)\times policy\_loss'. \]

## 预训练

预训练1 (Pre1): 使用随即代理对弈, 冻结策略网络, 只对价值网络进行训练, 使得价值网络具有基础的判断形势的能力 (如数子, 观察眼型).

预训练2 (Pre2): 使用平均分布作为先验概率进行树搜索, 并使用经典 ucb 公式防止结果搜索收敛, ( $ ucb =  $ ). 对策略网络和价值网络进行训练, 使网络能够注意到一些基本的构造 (如粘, 吃, 长, 尖, 飞...).

## 数据收集和增强

训练时, 训练代理应该收集每一步的棋局状态, 蒙特卡洛分布和价值估计, 以及终局的胜负. 一条正式经验是五元组 (t, z, mcts, q, l), 分别表示棋局状态编码得到的输入张量, 终局胜负, 蒙特卡洛分布, 蒙特卡洛价值估计, 总损失.

每一个局面和他的 7 个等价局面 (D4 对称群) 都会被收集到经验中.

一方面, 可以增强神经网络对对称情况的稳定性, 另一方面, 由于神经网络并不内禀具有对称性, 因此对称局面对其的训练效果是相对独立的, 第三方面, 就算神经网络已经建立了对称稳定性, 经验池的设计会使得重复多余经验迅速优先采样地位, 最后, 新经验的对称情形有利于增强有效经验的信号稳定性, 增强对有效经验的利用率.

## 噪声和随机

引入两种噪声来促进探索.

1. $\epsilon$-贪婪. 训练代理在完成蒙特卡洛演算后, 以 $\epsilon$ 的概率选择一个随即动作, $(1-\epsilon)$ 的概率选择蒙特卡洛访问次数最大的动作. $\epsilon$ 从训练初期的 0.5 逐步降低到0.001.

2. 迪利克雷噪声. 训练代理的先验概率将被设置为 $P(\cdot|s) + \eta \times noise$, 其中 $noise$ 服从强度为 $\alpha$ 的迪利克雷分布. $\alpha$ 取 0.03, $\eta$ 取 0.25.

## 异步推理架构

为了节省更新和预测, 编码, 以及 CPU-GPU 传输的时间, 使用两个线程同时给 GPU 喂数据, 并利用 GPU 的任务队列以及 pytorch 的异步阻塞机制, 完成计算和更新. 由一个主线程拥有决策树, 并处理来自两个推理线程的预测请求和更新请求.

预测时, 使用 '伪访问' 计数, 若一个预测进入到了已经进入计算流程的节点, 则回退, 并为当前路径添加一次伪访问, 结合伪访问并再次预测, 直至分支. 若伪访问次数大于节点总访问次数的 k 倍 (k < 0.1), 则失败, 把当前请求加入等待队列, 等待其它线程计算完毕, 再次进行预测, 防止无限增长的伪访问使明显不好的局面进入计算流程, 引入偏差. 计算完成后, 清除路径中的伪访问.

由于使用的 ucb 鼓励探索, 可以期望在已经过初步探索, 但未收敛的棋局中, 少量的访问次数增加即可使探索目标改变, 而一条搜索路径中这样的局面几乎总是存在的, 所以可以估计失败次数会随着探索次数增加快速减少, 最好情况下可以达到 100% GPU 负载; 最坏情况下, 由于 CPU 速度数量级上远超 GPU 计算速度, 算法只会退化到单线程预测-编码-计算-更新模型.

```text
         Master            Process A           Process B            GPU
    =========================================================================
         | | |               | | |               | | |               |
 success | P +===============+ P |               | | |               |
 success | P +============== | E | ==============+ P |               |
         | | |               | S +==== Task 1 == | E | ===========> ===
         | | |               | | |               | S + == Task 2 => |||
         | | |               | | |               | | |              |||
         | | |               | | |               | | |              ||| Task 1
         | | |               | | |               | | |              |||
         | U <===============+ U <==== Task 1 == | | | ===========+ ===
    fail | P <===============+ P |               | | |              |||
         | | |               | P |               | | |              |||
         | | |               | P |               | | |              ||| Task 2
         | | |               | P |               | | |              |||
         | | |               | P |               | | |              |||
         | U <============== | P | === Task 2 ===+ U <============+ ===
  reload | P +===============> P |               | P |               |
 success | P +============== | E | ==============+ P |               |
         | | |               | S +==== Task 3 == | E | ===========> ===
         | | |               | | |               | S + == Task 4 => |||
         | | |               | | |               | | |              |||
         | | |               | | |               | | |              ||| Task 3
         | | |               | | |               | | |              |||
         | | |               | | |               | | |              |||
         | U <===============+ U <==== Task 3 == | | | ===========+ ===
 success | P +===============+ P |               | | |              |||
         | | |               | E |               | | |              |||
         | | |               | S +==== Task 5 == | | | ===========> ||| Task 4
         | | |               | | |               | | |              |||
         | | |               | | |               | | |              |||
         | U <============== | | | === Task 4 ===+ U <============+ ===
    fail | P <============== | | | ==============+ P |              |||
         | | |               | | |               | P |              |||
         | | |               | | |               | P |              ||| Task 5
           .                   .                   .                 .
           .                   .                   .                 .
           .                   .                   .                 .
```
